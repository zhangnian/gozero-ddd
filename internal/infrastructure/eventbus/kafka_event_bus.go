package eventbus

import (
	"context"
	"encoding/json"
	"errors"
	"log"
	"sync"
	"time"

	"github.com/segmentio/kafka-go"

	"gozero-ddd/internal/domain/event"
)

// KafkaConfig Kafka é…ç½®
type KafkaConfig struct {
	Brokers       []string      // Kafka broker åœ°å€åˆ—è¡¨
	Topic         string        // äº‹ä»¶ä¸»é¢˜
	GroupID       string        // æ¶ˆè´¹è€…ç»„ID
	WriteTimeout  time.Duration // å†™å…¥è¶…æ—¶
	ReadTimeout   time.Duration // è¯»å–è¶…æ—¶
	BatchSize     int           // æ‰¹é‡å‘é€å¤§å°
	BatchTimeout  time.Duration // æ‰¹é‡å‘é€è¶…æ—¶
	RequiredAcks  int           // ç¡®è®¤æ¨¡å¼: -1=all, 0=none, 1=leader
	Async         bool          // æ˜¯å¦å¼‚æ­¥å‘é€
	AutoCreateTopic bool        // æ˜¯å¦è‡ªåŠ¨åˆ›å»ºä¸»é¢˜
}

// DefaultKafkaConfig é»˜è®¤é…ç½®
func DefaultKafkaConfig() KafkaConfig {
	return KafkaConfig{
		Brokers:       []string{"localhost:9092"},
		Topic:         "domain-events",
		GroupID:       "knowledge-service",
		WriteTimeout:  10 * time.Second,
		ReadTimeout:   10 * time.Second,
		BatchSize:     100,
		BatchTimeout:  time.Second,
		RequiredAcks:  -1, // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
		Async:         false,
		AutoCreateTopic: true,
	}
}

// ==================== äº‹ä»¶æ¶ˆæ¯ç»“æ„ ====================

// EventMessage Kafka æ¶ˆæ¯ç»“æ„
// ç”¨äºåºåˆ—åŒ–/ååºåˆ—åŒ–é¢†åŸŸäº‹ä»¶
type EventMessage struct {
	EventID     string          `json:"event_id"`
	EventName   string          `json:"event_name"`
	AggregateID string          `json:"aggregate_id"`
	OccurredAt  time.Time       `json:"occurred_at"`
	Payload     json.RawMessage `json:"payload"` // äº‹ä»¶å…·ä½“æ•°æ®
	Metadata    EventMetadata   `json:"metadata"`
}

// EventMetadata äº‹ä»¶å…ƒæ•°æ®
type EventMetadata struct {
	TraceID     string `json:"trace_id,omitempty"`
	ServiceName string `json:"service_name,omitempty"`
	Version     string `json:"version,omitempty"`
}

// ==================== Kafka äº‹ä»¶å‘å¸ƒå™¨ ====================

// KafkaEventPublisher Kafka äº‹ä»¶å‘å¸ƒå™¨
// è´Ÿè´£å°†é¢†åŸŸäº‹ä»¶å‘å¸ƒåˆ° Kafka
type KafkaEventPublisher struct {
	writer *kafka.Writer
	config KafkaConfig
	mu     sync.Mutex
}

// NewKafkaEventPublisher åˆ›å»º Kafka äº‹ä»¶å‘å¸ƒå™¨
func NewKafkaEventPublisher(config KafkaConfig) (*KafkaEventPublisher, error) {
	// è‡ªåŠ¨åˆ›å»º topicï¼ˆå¦‚æœå¯ç”¨ï¼‰
	if config.AutoCreateTopic {
		if err := createTopicIfNotExists(config); err != nil {
			log.Printf("âš ï¸ [Kafka] è‡ªåŠ¨åˆ›å»ºä¸»é¢˜å¤±è´¥: %v (å¯èƒ½ä¸»é¢˜å·²å­˜åœ¨)", err)
		}
	}

	writer := &kafka.Writer{
		Addr:         kafka.TCP(config.Brokers...),
		Topic:        config.Topic,
		Balancer:     &kafka.LeastBytes{}, // è´Ÿè½½å‡è¡¡ç­–ç•¥
		WriteTimeout: config.WriteTimeout,
		BatchSize:    config.BatchSize,
		BatchTimeout: config.BatchTimeout,
		RequiredAcks: kafka.RequiredAcks(config.RequiredAcks),
		Async:        config.Async,
	}

	log.Printf("ğŸ“¤ [Kafka] äº‹ä»¶å‘å¸ƒå™¨å·²åˆ›å»º: brokers=%v, topic=%s", config.Brokers, config.Topic)

	return &KafkaEventPublisher{
		writer: writer,
		config: config,
	}, nil
}

// createTopicIfNotExists å¦‚æœä¸»é¢˜ä¸å­˜åœ¨åˆ™åˆ›å»º
func createTopicIfNotExists(config KafkaConfig) error {
	conn, err := kafka.Dial("tcp", config.Brokers[0])
	if err != nil {
		return err
	}
	defer conn.Close()

	controller, err := conn.Controller()
	if err != nil {
		return err
	}

	controllerConn, err := kafka.Dial("tcp", controller.Host+":"+string(rune(controller.Port)))
	if err != nil {
		return err
	}
	defer controllerConn.Close()

	topicConfigs := []kafka.TopicConfig{
		{
			Topic:             config.Topic,
			NumPartitions:     3,  // åˆ†åŒºæ•°
			ReplicationFactor: 1,  // å‰¯æœ¬å› å­ï¼ˆå•æœºç¯å¢ƒè®¾ä¸º1ï¼‰
		},
	}

	return controllerConn.CreateTopics(topicConfigs...)
}

// ç¡®ä¿å®ç°äº†æ¥å£
var _ event.EventPublisher = (*KafkaEventPublisher)(nil)

// Publish å‘å¸ƒå•ä¸ªäº‹ä»¶åˆ° Kafka
func (p *KafkaEventPublisher) Publish(ctx context.Context, evt event.DomainEvent) error {
	return p.PublishAll(ctx, []event.DomainEvent{evt})
}

// PublishAll æ‰¹é‡å‘å¸ƒäº‹ä»¶åˆ° Kafka
func (p *KafkaEventPublisher) PublishAll(ctx context.Context, events []event.DomainEvent) error {
	if len(events) == 0 {
		return nil
	}

	messages := make([]kafka.Message, 0, len(events))
	
	for _, evt := range events {
		msg, err := p.buildMessage(ctx, evt)
		if err != nil {
			log.Printf("âŒ [Kafka] æ„å»ºæ¶ˆæ¯å¤±è´¥: %v", err)
			continue
		}
		messages = append(messages, msg)
	}

	if err := p.writer.WriteMessages(ctx, messages...); err != nil {
		log.Printf("âŒ [Kafka] å‘å¸ƒäº‹ä»¶å¤±è´¥: %v", err)
		return err
	}

	log.Printf("ğŸ“¤ [Kafka] æˆåŠŸå‘å¸ƒ %d ä¸ªäº‹ä»¶åˆ°ä¸»é¢˜ %s", len(messages), p.config.Topic)
	return nil
}

// buildMessage æ„å»º Kafka æ¶ˆæ¯
func (p *KafkaEventPublisher) buildMessage(ctx context.Context, evt event.DomainEvent) (kafka.Message, error) {
	// åºåˆ—åŒ–äº‹ä»¶æ•°æ®
	payload, err := json.Marshal(evt)
	if err != nil {
		return kafka.Message{}, err
	}

	// æ„å»ºäº‹ä»¶æ¶ˆæ¯
	eventMsg := EventMessage{
		EventID:     evt.EventID(),
		EventName:   evt.EventName(),
		AggregateID: evt.AggregateID(),
		OccurredAt:  evt.OccurredAt(),
		Payload:     payload,
		Metadata: EventMetadata{
			ServiceName: "knowledge-service",
			Version:     "1.0",
		},
	}

	// ä» context æå– trace_idï¼ˆå¦‚æœæœ‰ï¼‰
	if traceID, ok := ctx.Value("trace_id").(string); ok {
		eventMsg.Metadata.TraceID = traceID
	}

	value, err := json.Marshal(eventMsg)
	if err != nil {
		return kafka.Message{}, err
	}

	return kafka.Message{
		Key:   []byte(evt.AggregateID()), // ä½¿ç”¨èšåˆæ ¹IDä½œä¸ºåˆ†åŒºé”®ï¼Œä¿è¯åŒä¸€èšåˆçš„äº‹ä»¶æœ‰åº
		Value: value,
		Headers: []kafka.Header{
			{Key: "event_name", Value: []byte(evt.EventName())},
			{Key: "event_id", Value: []byte(evt.EventID())},
		},
	}, nil
}

// Close å…³é—­å‘å¸ƒå™¨
func (p *KafkaEventPublisher) Close() error {
	log.Println("ğŸ›‘ [Kafka] å…³é—­äº‹ä»¶å‘å¸ƒå™¨")
	return p.writer.Close()
}

// ==================== Kafka äº‹ä»¶æ¶ˆè´¹å™¨ ====================

// KafkaEventConsumer Kafka äº‹ä»¶æ¶ˆè´¹å™¨
// è´Ÿè´£ä» Kafka æ¶ˆè´¹é¢†åŸŸäº‹ä»¶å¹¶åˆ†å‘ç»™å¤„ç†å™¨
type KafkaEventConsumer struct {
	reader   *kafka.Reader
	config   KafkaConfig
	handlers map[string][]event.EventHandler // äº‹ä»¶å¤„ç†å™¨æ˜ å°„
	allHandlers []event.EventHandler          // å…¨å±€å¤„ç†å™¨
	mu       sync.RWMutex
	running  bool
	stopCh   chan struct{}
	wg       sync.WaitGroup
}

// NewKafkaEventConsumer åˆ›å»º Kafka äº‹ä»¶æ¶ˆè´¹å™¨
func NewKafkaEventConsumer(config KafkaConfig) *KafkaEventConsumer {
	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        config.Brokers,
		Topic:          config.Topic,
		GroupID:        config.GroupID,
		MinBytes:       10e3,        // 10KB
		MaxBytes:       10e6,        // 10MB
		MaxWait:        time.Second, // æœ€å¤§ç­‰å¾…æ—¶é—´
		StartOffset:    kafka.FirstOffset,
		CommitInterval: time.Second, // è‡ªåŠ¨æäº¤é—´éš”
	})

	log.Printf("ğŸ“¥ [Kafka] äº‹ä»¶æ¶ˆè´¹å™¨å·²åˆ›å»º: brokers=%v, topic=%s, group=%s",
		config.Brokers, config.Topic, config.GroupID)

	return &KafkaEventConsumer{
		reader:      reader,
		config:      config,
		handlers:    make(map[string][]event.EventHandler),
		allHandlers: make([]event.EventHandler, 0),
		stopCh:      make(chan struct{}),
	}
}

// ç¡®ä¿å®ç°äº†æ¥å£
var _ event.EventSubscriber = (*KafkaEventConsumer)(nil)

// Subscribe è®¢é˜…ç‰¹å®šäº‹ä»¶
func (c *KafkaEventConsumer) Subscribe(eventName string, handler event.EventHandler) {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.handlers[eventName] = append(c.handlers[eventName], handler)
	log.Printf("ğŸ“« [Kafka] æ³¨å†Œäº‹ä»¶å¤„ç†å™¨: %s", eventName)
}

// SubscribeAll è®¢é˜…æ‰€æœ‰äº‹ä»¶
func (c *KafkaEventConsumer) SubscribeAll(handler event.EventHandler) {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.allHandlers = append(c.allHandlers, handler)
	log.Printf("ğŸ“« [Kafka] æ³¨å†Œå…¨å±€äº‹ä»¶å¤„ç†å™¨")
}

// Start å¯åŠ¨æ¶ˆè´¹è€…
func (c *KafkaEventConsumer) Start(ctx context.Context) error {
	c.mu.Lock()
	if c.running {
		c.mu.Unlock()
		return errors.New("æ¶ˆè´¹è€…å·²åœ¨è¿è¡Œ")
	}
	c.running = true
	c.mu.Unlock()

	log.Println("ğŸš€ [Kafka] å¯åŠ¨äº‹ä»¶æ¶ˆè´¹è€…...")

	c.wg.Add(1)
	go c.consumeLoop(ctx)

	return nil
}

// consumeLoop æ¶ˆè´¹å¾ªç¯
func (c *KafkaEventConsumer) consumeLoop(ctx context.Context) {
	defer c.wg.Done()

	for {
		select {
		case <-ctx.Done():
			log.Println("ğŸ›‘ [Kafka] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºæ¶ˆè´¹å¾ªç¯")
			return
		case <-c.stopCh:
			log.Println("ğŸ›‘ [Kafka] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºæ¶ˆè´¹å¾ªç¯")
			return
		default:
			msg, err := c.reader.ReadMessage(ctx)
			if err != nil {
				if errors.Is(err, context.Canceled) {
					return
				}
				log.Printf("âŒ [Kafka] è¯»å–æ¶ˆæ¯å¤±è´¥: %v", err)
				continue
			}

			c.handleMessage(ctx, msg)
		}
	}
}

// handleMessage å¤„ç† Kafka æ¶ˆæ¯
func (c *KafkaEventConsumer) handleMessage(ctx context.Context, msg kafka.Message) {
	var eventMsg EventMessage
	if err := json.Unmarshal(msg.Value, &eventMsg); err != nil {
		log.Printf("âŒ [Kafka] è§£ææ¶ˆæ¯å¤±è´¥: %v", err)
		return
	}

	log.Printf("ğŸ“¥ [Kafka] æ”¶åˆ°äº‹ä»¶: %s, EventID=%s, AggregateID=%s",
		eventMsg.EventName, eventMsg.EventID, eventMsg.AggregateID)

	// åˆ›å»ºåŒ…è£…çš„äº‹ä»¶å¯¹è±¡
	wrappedEvent := &WrappedDomainEvent{
		eventMsg: eventMsg,
	}

	// è°ƒç”¨å¤„ç†å™¨
	c.dispatchEvent(ctx, eventMsg.EventName, wrappedEvent)
}

// dispatchEvent åˆ†å‘äº‹ä»¶ç»™å¤„ç†å™¨
func (c *KafkaEventConsumer) dispatchEvent(ctx context.Context, eventName string, evt event.DomainEvent) {
	c.mu.RLock()
	defer c.mu.RUnlock()

	// è°ƒç”¨ç‰¹å®šäº‹ä»¶å¤„ç†å™¨
	if handlers, ok := c.handlers[eventName]; ok {
		for _, handler := range handlers {
			if err := handler.Handle(ctx, evt); err != nil {
				log.Printf("âŒ [Kafka] äº‹ä»¶å¤„ç†å¤±è´¥: %s, é”™è¯¯: %v", eventName, err)
			}
		}
	}

	// è°ƒç”¨å…¨å±€å¤„ç†å™¨
	for _, handler := range c.allHandlers {
		if err := handler.Handle(ctx, evt); err != nil {
			log.Printf("âŒ [Kafka] å…¨å±€å¤„ç†å™¨æ‰§è¡Œå¤±è´¥: %v", err)
		}
	}
}

// Stop åœæ­¢æ¶ˆè´¹è€…
func (c *KafkaEventConsumer) Stop() error {
	c.mu.Lock()
	if !c.running {
		c.mu.Unlock()
		return nil
	}
	c.running = false
	c.mu.Unlock()

	close(c.stopCh)
	c.wg.Wait()

	log.Println("ğŸ›‘ [Kafka] å…³é—­äº‹ä»¶æ¶ˆè´¹å™¨")
	return c.reader.Close()
}

// ==================== åŒ…è£…äº‹ä»¶ ====================

// WrappedDomainEvent åŒ…è£…çš„é¢†åŸŸäº‹ä»¶
// ç”¨äºä» Kafka æ¶ˆæ¯ååºåˆ—åŒ–åçš„äº‹ä»¶
type WrappedDomainEvent struct {
	eventMsg EventMessage
}

func (e *WrappedDomainEvent) EventID() string {
	return e.eventMsg.EventID
}

func (e *WrappedDomainEvent) EventName() string {
	return e.eventMsg.EventName
}

func (e *WrappedDomainEvent) OccurredAt() time.Time {
	return e.eventMsg.OccurredAt
}

func (e *WrappedDomainEvent) AggregateID() string {
	return e.eventMsg.AggregateID
}

// Payload è·å–åŸå§‹äº‹ä»¶æ•°æ®
func (e *WrappedDomainEvent) Payload() json.RawMessage {
	return e.eventMsg.Payload
}

// Metadata è·å–äº‹ä»¶å…ƒæ•°æ®
func (e *WrappedDomainEvent) Metadata() EventMetadata {
	return e.eventMsg.Metadata
}

// ==================== Kafka äº‹ä»¶æ€»çº¿ ====================

// KafkaEventBus Kafka äº‹ä»¶æ€»çº¿
// ç»„åˆäº†å‘å¸ƒå™¨å’Œæ¶ˆè´¹å™¨ï¼Œå®ç°å®Œæ•´çš„äº‹ä»¶æ€»çº¿åŠŸèƒ½
type KafkaEventBus struct {
	publisher *KafkaEventPublisher
	consumer  *KafkaEventConsumer
}

// NewKafkaEventBus åˆ›å»º Kafka äº‹ä»¶æ€»çº¿
func NewKafkaEventBus(config KafkaConfig) (*KafkaEventBus, error) {
	publisher, err := NewKafkaEventPublisher(config)
	if err != nil {
		return nil, err
	}

	consumer := NewKafkaEventConsumer(config)

	return &KafkaEventBus{
		publisher: publisher,
		consumer:  consumer,
	}, nil
}

// ç¡®ä¿å®ç°äº†æ¥å£
var _ event.EventBus = (*KafkaEventBus)(nil)

// Publish å‘å¸ƒäº‹ä»¶
func (b *KafkaEventBus) Publish(ctx context.Context, evt event.DomainEvent) error {
	return b.publisher.Publish(ctx, evt)
}

// PublishAll æ‰¹é‡å‘å¸ƒäº‹ä»¶
func (b *KafkaEventBus) PublishAll(ctx context.Context, events []event.DomainEvent) error {
	return b.publisher.PublishAll(ctx, events)
}

// Subscribe è®¢é˜…ç‰¹å®šäº‹ä»¶
func (b *KafkaEventBus) Subscribe(eventName string, handler event.EventHandler) {
	b.consumer.Subscribe(eventName, handler)
}

// SubscribeAll è®¢é˜…æ‰€æœ‰äº‹ä»¶
func (b *KafkaEventBus) SubscribeAll(handler event.EventHandler) {
	b.consumer.SubscribeAll(handler)
}

// Start å¯åŠ¨äº‹ä»¶æ€»çº¿ï¼ˆå¼€å§‹æ¶ˆè´¹äº‹ä»¶ï¼‰
func (b *KafkaEventBus) Start(ctx context.Context) error {
	return b.consumer.Start(ctx)
}

// Close å…³é—­äº‹ä»¶æ€»çº¿
func (b *KafkaEventBus) Close() error {
	if err := b.consumer.Stop(); err != nil {
		log.Printf("âŒ [Kafka] å…³é—­æ¶ˆè´¹å™¨å¤±è´¥: %v", err)
	}
	return b.publisher.Close()
}

